# LLM-Test-case-generator

# ğŸ§ª AI-Powered Test Case Generator

An internal-use prototype built to generate test cases using an LLM (Google Gemma) based on user prompts. Designed with an async job pipeline to avoid blocking the frontend and deliver real-time job status updates through HTTP polling.

> âš ï¸ This is a **reconstructed version** of an internal tool I initiated  for potential QA automation adoption.

---

## ğŸš€ Key Features

- ğŸ§  Accepts user prompt or problem statement input
- ğŸ¤– Uses **Google Gemma** (LLM) to generate relevant test cases
- ğŸ•’ Async job queue on backend to prevent UI blocking
- ğŸ” HTTP polling from frontend to check for job status and retrieve results
- ğŸ¯ Designed for fast, testable UX feedback loop

---

## ğŸ§© System Architecture


React.js (Frontend)
  â”œâ”€â”€ Submits prompt via POST
  â””â”€â”€ Polls backend for job status (HTTP)

Flask (Backend)
  â”œâ”€â”€ Receives request and queues LLM job
  â”œâ”€â”€ Runs async task (thread/queue) to call Google Gemma
  â””â”€â”€ On completion, stores result and exposes via GET


---


## âš™ï¸ Tech Stack

- **Frontend**: React.js (Vite)
- **Backend**: Python Flask
- **LLM**: Google Gemma (text-generation model)
- **Task Management**: Python threading & job queue
- **Communication**: HTTP polling (long-poll fallback)

---

## ğŸ› ï¸ Local Setup

```bash
# Clone the repo
git clone https://github.com/ArjunR00T/llm-testgen
cd llm-testgen

# Start backend
cd backend
pip install -r requirements.txt
python run.py

# Start frontend
cd ../frontend
npm install
npm run dev
```

ğŸ“Œ Notes
Current version uses threading for async handling â€” can be upgraded to Celery or RQ for production-scale job queues.

LLM integration is abstracted behind a generate_test_case() module for model flexibility.

Designed during internal R&D 

### ğŸ™Œ Author
Built by Arjun Gireesh
Backend Developer | Microservices â€¢ Queues â€¢ Async Architectures
Now open to backend engineering roles at product-focused teams.

